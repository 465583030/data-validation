{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Data Validation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes how to explore and validate Chicago Taxi dataset using TensorFlow Data Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages and set up data paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "TRAIN_DATA = os.path.join(DATA_DIR, 'train.csv')\n",
    "EVAL_DATA = os.path.join(DATA_DIR, 'eval.csv')\n",
    "SERVING_DATA = os.path.join(DATA_DIR, 'serving.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute descriptive data statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFDV can compute descriptive\n",
    "[statistics](https://github.com/tensorflow/metadata/tree/v0.6.0/tensorflow_metadata/proto/v0/statistics.proto)\n",
    "that provide a quick overview of the data in terms of the features that are\n",
    "present and the shapes of their value distributions.\n",
    "\n",
    "Internally, TFDV uses [Apache Beam](https://beam.apache.org)'s data-parallel\n",
    "processing framework to scale the computation of statistics over large datasets.\n",
    "For applications that wish to integrate deeper with TFDV (e.g., attach\n",
    "statistics generation at the end of a data-generation pipeline), the API also\n",
    "exposes a Beam PTransform for statistics generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = tfdv.generate_statistics_from_csv(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics can be visualized using [Facets Overview](https://pair-code.github.io/facets/) tool which provide a succinct visualization of these statistics for easy browsing. TFDV provides a utility method that visualizes statistics using Facets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer a schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The\n",
    "[schema](https://github.com/tensorflow/metadata/tree/v0.6.0/tensorflow_metadata/proto/v0/schema.proto)\n",
    "describes the expected properties of the data. Some of these properties are:\n",
    "\n",
    "*   which features are expected to be present\n",
    "*   their type\n",
    "*   the number of values for a feature in each example\n",
    "*   the presence of each feature across all examples\n",
    "*   the expected domains of features.\n",
    "\n",
    "In short, the schema describes the expectations for \"correct\" data and can thus\n",
    "be used to detect errors in the data (described below). \n",
    "\n",
    "Since writing a schema can be a tedious task, especially for datasets with lots\n",
    "of features, TFDV provides a method to generate an initial version of the schema\n",
    "based on the descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = tfdv.infer_schema(train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, TFDV uses conservative heuristics to infer stable data properties\n",
    "from the statistics in order to avoid overfitting the schema to the specific\n",
    "dataset. It is strongly advised to **review the inferred schema and refine\n",
    "it as needed**, to capture any domain knowledge about the data that TFDV's\n",
    "heuristics might have missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check evaluation data for errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a schema, it is possible to check whether a dataset conforms to the\n",
    "expectations set in the schema or whether there exist any data anomalies. TFDV\n",
    "performs this check by matching the statistics of the dataset against the schema\n",
    "and marking any discrepancies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute stats over eval data.\n",
    "eval_stats = tfdv.generate_statistics_from_csv(EVAL_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare stats of eval data with training data.\n",
    "tfdv.visualize_statistics(lhs_statistics=eval_stats, rhs_statistics=train_stats,\n",
    "                          lhs_name='EVAL_DATASET', rhs_name='TRAIN_DATASET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check eval data for errors by validating the eval data stats using the previously inferred schema.\n",
    "anomalies = tfdv.validate_statistics(eval_stats, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_anomalies(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The anomalies indicate that out of domain values were found for features `company` and `payment_type` in the stats in < 1% of the examples. If this was expected, then the schema can be updated as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relax the minimum fraction of values that must come from the domain for feature company.\n",
    "company = tfdv.get_feature(schema, 'company')\n",
    "company.distribution_constraints.min_domain_mass = 0.9\n",
    "\n",
    "# Add new value to the domain of feature payment_type.\n",
    "payment_type_domain = tfdv.get_domain(schema, 'payment_type')\n",
    "payment_type_domain.value.append('Prcard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_anomalies = tfdv.validate_statistics(eval_stats, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_anomalies(updated_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an anomaly truly indicates a data error, then the underlying data should be fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, validations assume that all datasets in a pipeline adhere to a single schema. In some cases introducing\n",
    "slight schema variations is necessary, for instance features used as labels are required during training (and should\n",
    "be validated), but are missing during serving.\n",
    "\n",
    "**Environments** can be used to express such requirements. In particular, features in schema can be associated with a set of environments using `default_environment`, `in_environment` and `not_in_environment`.\n",
    "\n",
    "For example, if the `tips` feature is being used as the label in training, but missing in the serving data. Without environment specified, it will show up as an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_stats = tfdv.generate_statistics_from_csv(SERVING_DATA)\n",
    "serving_anomalies = tfdv.validate_statistics(serving_stats, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_anomalies(serving_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that 'tips' feature is shown in the anomalies as 'Column dropped', as it is not present in the serving dataset. We can do the following to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All features are by default in both TRAINING and SERVING environments.\n",
    "schema.default_environment.append('TRAINING')\n",
    "schema.default_environment.append('SERVING')\n",
    "\n",
    "# Specify that 'tips' feature is not in SERVING environment.\n",
    "tfdv.get_feature(schema, 'tips').not_in_environment.append('SERVING')\n",
    "\n",
    "serving_anomalies_with_env = tfdv.validate_statistics(\n",
    "    serving_stats, schema, environment='SERVING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_anomalies(serving_anomalies_with_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check data drift and skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to checking whether a dataset conforms to the expectations set in the schema, TFDV also provides functionalities to detect\n",
    "* drift between different days of training data\n",
    "* skew between training and serving data\n",
    "\n",
    "TFDV performs this check by comparing the statistics of different datasets based on the drift/skew comparators specified in the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add skew comparator for 'payment_type' feature.\n",
    "payment_type = tfdv.get_feature(schema, 'payment_type')\n",
    "payment_type.skew_comparator.infinity_norm.threshold = 0.01\n",
    "\n",
    "# Add drift comparator for 'company' feature.\n",
    "company=tfdv.get_feature(schema, 'company')\n",
    "company.drift_comparator.infinity_norm.threshold = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_anomalies = tfdv.validate_statistics(train_stats, schema, previous_statistics=eval_stats,\n",
    "                                          serving_statistics=serving_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_anomalies(skew_anomalies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
